{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, concat, slice, stack, tensor, tidy, unstack, util, io, Tensor, add, addN, mod, mul, div, floorDiv, sub, minimum, maximum, pow, squaredDifference, abs, acos, acosh, asin, asinh, atan, atan2, atanh, ceil, complex, cos, cosh, elu, erf, exp, expm1, floor, log, log1p, imag, neg, reciprocal, real, relu, round, selu, sigmoid, sin, sign, sinh, softplus, sqrt, square, tanh, tan, clipByValue, rsqrt, prod, leakyRelu, scalar, conv1d, conv2d, fused, conv2dTranspose, depthwiseConv2d, conv3d, avgPool, maxPool, avgPool3d, maxPool3d, fill, linspace, multinomial, oneHot, ones, onesLike, randomUniform, range, truncatedNormal, zeros, zerosLike, image, whereAsync, setdiff1dAsync, topk, tensor1d, equal, notEqual, greater, greaterEqual, less, lessEqual, logicalAnd, logicalNot, logicalOr, where, matMul, transpose, batchNorm, localResponseNormalization, softmax, logSoftmax, sparseToDense, max, mean, min, sum, all, any, argMax, argMin, gather, reverse, stridedSlice, tile, split, scatterND, gatherND, fft, ifft, rfft, irfft, cast, expandDims, squeeze, reshape, pad, spaceToBatchND, batchToSpaceND, depthToSpace } from \"@tensorflow/tfjs-core\";\n\nvar DataType,\n    SaverDef,\n    _assign = function __assign() {\n  return (_assign = Object.assign || function (e) {\n    for (var t, a = 1, r = arguments.length; a < r; a++) {\n      for (var n in t = arguments[a]) {\n        Object.prototype.hasOwnProperty.call(t, n) && (e[n] = t[n]);\n      }\n    }\n\n    return e;\n  }).apply(this, arguments);\n};\n\nfunction __awaiter(e, t, a, r) {\n  return new (a || (a = Promise))(function (n, s) {\n    function o(e) {\n      try {\n        u(r.next(e));\n      } catch (e) {\n        s(e);\n      }\n    }\n\n    function p(e) {\n      try {\n        u(r.throw(e));\n      } catch (e) {\n        s(e);\n      }\n    }\n\n    function u(e) {\n      e.done ? n(e.value) : new a(function (t) {\n        t(e.value);\n      }).then(o, p);\n    }\n\n    u((r = r.apply(e, t || [])).next());\n  });\n}\n\nfunction __generator(e, t) {\n  var a,\n      r,\n      n,\n      s,\n      o = {\n    label: 0,\n    sent: function sent() {\n      if (1 & n[0]) throw n[1];\n      return n[1];\n    },\n    trys: [],\n    ops: []\n  };\n  return s = {\n    next: p(0),\n    throw: p(1),\n    return: p(2)\n  }, \"function\" == typeof Symbol && (s[Symbol.iterator] = function () {\n    return this;\n  }), s;\n\n  function p(s) {\n    return function (p) {\n      return function (s) {\n        if (a) throw new TypeError(\"Generator is already executing.\");\n\n        for (; o;) {\n          try {\n            if (a = 1, r && (n = 2 & s[0] ? r.return : s[0] ? r.throw || ((n = r.return) && n.call(r), 0) : r.next) && !(n = n.call(r, s[1])).done) return n;\n\n            switch (r = 0, n && (s = [2 & s[0], n.value]), s[0]) {\n              case 0:\n              case 1:\n                n = s;\n                break;\n\n              case 4:\n                return o.label++, {\n                  value: s[1],\n                  done: !1\n                };\n\n              case 5:\n                o.label++, r = s[1], s = [0];\n                continue;\n\n              case 7:\n                s = o.ops.pop(), o.trys.pop();\n                continue;\n\n              default:\n                if (!(n = (n = o.trys).length > 0 && n[n.length - 1]) && (6 === s[0] || 2 === s[0])) {\n                  o = 0;\n                  continue;\n                }\n\n                if (3 === s[0] && (!n || s[1] > n[0] && s[1] < n[3])) {\n                  o.label = s[1];\n                  break;\n                }\n\n                if (6 === s[0] && o.label < n[1]) {\n                  o.label = n[1], n = s;\n                  break;\n                }\n\n                if (n && o.label < n[2]) {\n                  o.label = n[2], o.ops.push(s);\n                  break;\n                }\n\n                n[2] && o.ops.pop(), o.trys.pop();\n                continue;\n            }\n\n            s = t.call(e, o);\n          } catch (e) {\n            s = [6, e], r = 0;\n          } finally {\n            a = n = 0;\n          }\n        }\n\n        if (5 & s[0]) throw s[1];\n        return {\n          value: s[0] ? s[1] : void 0,\n          done: !0\n        };\n      }([s, p]);\n    };\n  }\n}\n\n!function (e) {\n  e[e.DT_INVALID = 0] = \"DT_INVALID\", e[e.DT_FLOAT = 1] = \"DT_FLOAT\", e[e.DT_DOUBLE = 2] = \"DT_DOUBLE\", e[e.DT_INT32 = 3] = \"DT_INT32\", e[e.DT_UINT8 = 4] = \"DT_UINT8\", e[e.DT_INT16 = 5] = \"DT_INT16\", e[e.DT_INT8 = 6] = \"DT_INT8\", e[e.DT_STRING = 7] = \"DT_STRING\", e[e.DT_COMPLEX64 = 8] = \"DT_COMPLEX64\", e[e.DT_INT64 = 9] = \"DT_INT64\", e[e.DT_BOOL = 10] = \"DT_BOOL\", e[e.DT_QINT8 = 11] = \"DT_QINT8\", e[e.DT_QUINT8 = 12] = \"DT_QUINT8\", e[e.DT_QINT32 = 13] = \"DT_QINT32\", e[e.DT_BFLOAT16 = 14] = \"DT_BFLOAT16\", e[e.DT_FLOAT_REF = 101] = \"DT_FLOAT_REF\", e[e.DT_DOUBLE_REF = 102] = \"DT_DOUBLE_REF\", e[e.DT_INT32_REF = 103] = \"DT_INT32_REF\", e[e.DT_UINT8_REF = 104] = \"DT_UINT8_REF\", e[e.DT_INT16_REF = 105] = \"DT_INT16_REF\", e[e.DT_INT8_REF = 106] = \"DT_INT8_REF\", e[e.DT_STRING_REF = 107] = \"DT_STRING_REF\", e[e.DT_COMPLEX64_REF = 108] = \"DT_COMPLEX64_REF\", e[e.DT_INT64_REF = 109] = \"DT_INT64_REF\", e[e.DT_BOOL_REF = 110] = \"DT_BOOL_REF\", e[e.DT_QINT8_REF = 111] = \"DT_QINT8_REF\", e[e.DT_QUINT8_REF = 112] = \"DT_QUINT8_REF\", e[e.DT_QINT32_REF = 113] = \"DT_QINT32_REF\", e[e.DT_BFLOAT16_REF = 114] = \"DT_BFLOAT16_REF\";\n}(DataType || (DataType = {})), function (e) {\n  !function (e) {\n    e[e.LEGACY = 0] = \"LEGACY\", e[e.V1 = 1] = \"V1\", e[e.V2 = 2] = \"V2\";\n  }(e.CheckpointFormatVersion || (e.CheckpointFormatVersion = {}));\n}(SaverDef || (SaverDef = {}));\nvar CUSTOM_OPS = {};\n\nfunction registerOp(e, t) {\n  var a = {\n    tfOpName: e,\n    category: \"custom\",\n    inputs: [],\n    attrs: [],\n    customExecutor: t\n  };\n  CUSTOM_OPS[e] = a;\n}\n\nfunction getRegisteredOp(e) {\n  return CUSTOM_OPS[e];\n}\n\nfunction deregisterOp(e) {\n  delete CUSTOM_OPS[e];\n}\n\nfunction getParamValue(e, t, a, r) {\n  var n = t.inputParams[e];\n\n  if (n && void 0 !== n.inputIndexStart) {\n    var s = n.inputIndexStart,\n        o = 0 === n.inputIndexEnd ? void 0 : void 0 === n.inputIndexEnd ? s + 1 : n.inputIndexEnd;\n    if (\"tensor\" === n.type) return getTensor(t.inputNames[n.inputIndexStart], a, r);\n    if (\"tensors\" === n.type) return t.inputNames.slice(s, o).map(function (e) {\n      return getTensor(e, a, r);\n    });\n    var p = Array.prototype.slice.call(getTensor(t.inputNames.slice(s)[0], a, r).dataSync());\n    return \"number\" === n.type ? p[0] : p;\n  }\n\n  var u = t.attrParams[e];\n  return u && u.value;\n}\n\nfunction getTensor(e, t, a) {\n  var r = parseNodeName(e),\n      n = r[0],\n      s = r[1],\n      o = a.currentContextIds.find(function (e) {\n    return !!t[getNodeNameWithContextId(n, e)];\n  });\n  return void 0 !== o ? t[getNodeNameWithContextId(n, o)][s] : void 0;\n}\n\nfunction getTensorsForCurrentContenxt(e, t, a) {\n  return t[getNodeNameWithContextId(e, a.currentContextId)];\n}\n\nfunction getNodeNameAndIndex(e, t) {\n  var a = parseNodeName(e),\n      r = a[0],\n      n = a[1];\n  return [getNodeNameWithContextId(r, t && t.currentContextId), n];\n}\n\nfunction getNodeNameWithContextId(e, t) {\n  return t ? e + \"-\" + t : e;\n}\n\nfunction parseNodeName(e) {\n  var t = e.lastIndexOf(\":\");\n  return -1 === t ? [e, 0] : [e.substring(0, t), Number(e.substring(t + 1))];\n}\n\nfunction split$1(e, t) {\n  for (var a = [], r = 0; r < e.length; r += t) {\n    a.push(e.slice(r, r + t));\n  }\n\n  return a;\n}\n\nvar json = [{\n  tfOpName: \"Add\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AddV2\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AddN\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"BiasAdd\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sub\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"RealDiv\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Div\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FloorDiv\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Mul\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Maximum\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Minimum\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Pow\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"SquaredDifference\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Mod\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FloorMod\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}],\n    arithmetic = Object.freeze({\n  json: json\n}),\n    json$1 = [{\n  tfOpName: \"Abs\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Acos\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Asin\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atan\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atan2\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"y\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Ceil\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ClipByValue\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"clip_value_min\",\n    name: \"clipValueMin\",\n    type: \"number\"\n  }, {\n    tfName: \"clip_value_max\",\n    name: \"clipValueMax\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Complex\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"real\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"imag\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ComplexAbs\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Cos\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Cosh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Elu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Exp\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Floor\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Log\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Imag\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"outputType\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Neg\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Real\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"outputType\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Prelu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"alpha\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Relu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Relu6\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"clipValueMin\",\n    name: \"clipValueMin\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"clipValueMax\",\n    name: \"clipValueMax\",\n    type: \"number\",\n    defaultValue: 6\n  }]\n}, {\n  tfOpName: \"Selu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sigmoid\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sin\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sinh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sqrt\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Rsqrt\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Square\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tan\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tanh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sign\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Round\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Expm1\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Log1p\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Reciprocal\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Softplus\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Asinh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Acosh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atanh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Erf\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Prod\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axes\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\",\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LeakyRelu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"alpha\",\n    name: \"alpha\",\n    type: \"number\",\n    defaultValue: .2\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}],\n    basicMath = Object.freeze({\n  json: json$1\n}),\n    json$2 = [{\n  tfOpName: \"LoopCond\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"pred\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Switch\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"data\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"pred\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Merge\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Enter\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"frame_name\",\n    name: \"frameName\",\n    type: \"string\"\n  }, {\n    tfName: \"is_constant\",\n    name: \"isConstant\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Exit\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"NextIteration\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"size\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape\",\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dynamic_size\",\n    name: \"dynamicSize\",\n    type: \"bool\"\n  }, {\n    tfName: \"clear_after_read\",\n    name: \"clearAfterRead\",\n    type: \"bool\"\n  }, {\n    tfName: \"identical_element_shapes\",\n    name: \"identicalElementShapes\",\n    type: \"bool\"\n  }, {\n    tfName: \"tensor_array_name\",\n    name: \"name\",\n    type: \"string\"\n  }]\n}, {\n  tfOpName: \"TensorArrayWriteV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayReadV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayGatherV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape\",\n    name: \"elementShape\",\n    type: \"shape\"\n  }]\n}, {\n  tfOpName: \"TensorArrayScatterV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorArrayConcatV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape_except0\",\n    name: \"elementShapeExcept0\",\n    type: \"shape\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArraySplitV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"lengths\",\n    type: \"number[]\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorArraySizeV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"flowIn\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"TensorArrayCloseV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"number\"\n  }]\n}],\n    control = Object.freeze({\n  json: json$2\n}),\n    json$3 = [{\n  tfOpName: \"AvgPool\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MaxPool\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AvgPool3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MaxPool3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Conv1D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"stride\",\n    name: \"stride\",\n    type: \"number\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NWC\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"dilation\",\n    name: \"dilation\",\n    type: \"number\",\n    defaultValue: 1\n  }]\n}, {\n  tfOpName: \"Conv2D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"useCudnnOnGpu\",\n    name: \"useCudnnOnGpu\",\n    type: \"bool\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"_FusedConv2D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"num_args\",\n    name: \"numArgs\",\n    type: \"number\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"use_cudnn_on_gpu\",\n    name: \"useCudnnOnGpu\",\n    type: \"bool\",\n    defaultValue: !0\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\",\n    defaultValue: [1, 1, 1, 1]\n  }, {\n    tfName: \"fused_ops\",\n    name: \"fusedOps\",\n    type: \"string[]\",\n    defaultValue: []\n  }, {\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: 1e-4\n  }]\n}, {\n  tfOpName: \"Conv2DBackpropInput\",\n  category: \"convolution\",\n  inputs: [{\n    start: 2,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }, {\n    start: 0,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"DepthwiseConv2d\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"DepthwiseConv2dNative\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Conv3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}],\n    convolution = Object.freeze({\n  json: json$3\n}),\n    json$4 = [{\n  tfOpName: \"Fill\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }, {\n    start: 1,\n    name: \"value\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"LinSpace\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"start\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"stop\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"num\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"OneHot\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"depth\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"onValue\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    start: 3,\n    name: \"offValue\",\n    type: \"number\",\n    defaultValue: 0\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Ones\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"OnesLike\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"RandomUniform\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"minval\",\n    name: \"minval\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"maxval\",\n    name: \"maxval\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"T\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Range\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"start\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"stop\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"step\",\n    type: \"number\",\n    defaultValue: 0\n  }],\n  attrs: [{\n    tfName: \"Tidx\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TruncatedNormal\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"means\",\n    name: \"mean\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"stddev\",\n    name: \"stdDev\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\"\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"T\",\n    name: \"T\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Zeros\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"ZerosLike\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Multinomial\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"logits\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"numSamples\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\"\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"output_dtype\",\n    name: \"output_dtype\",\n    type: \"dtype\"\n  }]\n}],\n    creation = Object.freeze({\n  json: json$4\n}),\n    json$5 = [{\n  tfOpName: \"NonMaxSuppressionV2\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"NonMaxSuppressionV3\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }, {\n    start: 4,\n    name: \"scoreThreshold\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Where\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"condition\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ListDiff\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"y\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}],\n    dynamic = Object.freeze({\n  json: json$5\n}),\n    json$6 = [{\n  tfOpName: \"TopKV2\",\n  category: \"evaluation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"k\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"sorted\",\n    name: \"sorted\",\n    type: \"bool\"\n  }]\n}],\n    evaluation = Object.freeze({\n  json: json$6\n}),\n    json$7 = [{\n  tfOpName: \"PlaceholderWithDefault\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"default\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"shape\",\n    name: \"shape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Placeholder\",\n  category: \"graph\",\n  attrs: [{\n    tfName: \"shape\",\n    name: \"shape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Const\",\n  category: \"graph\"\n}, {\n  tfOpName: \"Identity\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"IdentityN\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"x\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Snapshot\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Rank\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Size\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Shape\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"ShapeN\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"x\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Print\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"data\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"message\",\n    name: \"message\",\n    type: \"string\"\n  }, {\n    tfName: \"first_n\",\n    name: \"firstN\",\n    type: \"number\",\n    notSupported: !0\n  }, {\n    tfName: \"summarize\",\n    name: \"summarize\",\n    type: \"number\",\n    defaultValue: 3\n  }]\n}, {\n  tfOpName: \"NoOp\",\n  category: \"graph\",\n  inputs: []\n}, {\n  tfOpName: \"StopGradient\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"FakeQuantWithMinMaxVars\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"min\",\n    name: \"min\",\n    type: \"number\"\n  }, {\n    tfName: \"max\",\n    name: \"max\",\n    type: \"number\"\n  }]\n}],\n    graph = Object.freeze({\n  json: json$7\n}),\n    json$8 = [{\n  tfOpName: \"ResizeBilinear\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"images\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"align_corners\",\n    name: \"alignCorners\",\n    type: \"bool\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ResizeNearestNeighbor\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"images\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"align_corners\",\n    name: \"alignCorners\",\n    type: \"bool\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"CropAndResize\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"image\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"boxInd\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"cropSize\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"method\",\n    name: \"method\",\n    type: \"string\"\n  }, {\n    tfName: \"extrapolation_value\",\n    name: \"extrapolationValue\",\n    type: \"number\"\n  }]\n}],\n    image$1 = Object.freeze({\n  json: json$8\n}),\n    json$9 = [{\n  tfOpName: \"Equal\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"NotEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Greater\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"GreaterEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Less\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LessEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalAnd\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalNot\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalOr\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Select\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"condition\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}],\n    logical = Object.freeze({\n  json: json$9\n}),\n    json$10 = [{\n  tfOpName: \"MatMul\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"transpose_a\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"transpose_b\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"BatchMatMul\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"adj_x\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"adj_y\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"BatchMatMulV2\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"adj_x\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"adj_y\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Transpose\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"perm\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}],\n    matrices = Object.freeze({\n  json: json$10\n}),\n    json$11 = [{\n  tfOpName: \"FusedBatchNorm\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FusedBatchNormV2\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FusedBatchNormV3\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LRN\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"depth_radius\",\n    name: \"radius\",\n    type: \"number\",\n    defaultValue: 5\n  }, {\n    tfName: \"bias\",\n    name: \"bias\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"alpha\",\n    name: \"alpha\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"beta\",\n    name: \"beta\",\n    type: \"number\",\n    defaultValue: .5\n  }]\n}, {\n  tfOpName: \"Softmax\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"LogSoftmax\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseToDense\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"sparseIndices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"sparseValues\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    defaultValue: !0,\n    notSupported: !0\n  }]\n}],\n    normalization = Object.freeze({\n  json: json$11\n}),\n    json$12 = [{\n  tfOpName: \"Max\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Mean\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Min\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Sum\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"All\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Any\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"ArgMax\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"ArgMin\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Prod\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}],\n    reduction = Object.freeze({\n  json: json$12\n}),\n    json$13 = [{\n  tfOpName: \"ConcatV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    end: -1,\n    name: \"tensors\",\n    type: \"tensors\"\n  }, {\n    start: -1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Concat\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 1,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }, {\n    start: 0,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"GatherV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Gather\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Reverse\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"dims\",\n    type: \"bool\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ReverseV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Slice\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"begin\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"size\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"StridedSlice\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"begin\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"end\",\n    type: \"number[]\"\n  }, {\n    start: 3,\n    name: \"strides\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"begin_mask\",\n    name: \"beginMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"end_mask\",\n    name: \"endMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"new_axis_mask\",\n    name: \"newAxisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"ellipsis_mask\",\n    name: \"ellipsisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"shrink_axis_mask\",\n    name: \"shrinkAxisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Pack\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Unpack\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"num\",\n    name: \"num\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tile\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"reps\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Split\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    start: 1,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"num_split\",\n    name: \"numOrSizeSplits\",\n    type: \"number\",\n    defaultValue: 1\n  }]\n}, {\n  tfOpName: \"SplitV\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"numOrSizeSplits\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"ScatterNd\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"values\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"shape\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"GatherNd\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseToDense\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"sparseIndices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"sparseValues\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    defaultValue: !1,\n    notSupported: !0\n  }]\n}],\n    sliceJoin = Object.freeze({\n  json: json$13\n}),\n    json$14 = [{\n  tfOpName: \"FFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"IFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"RFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"fft_length\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"IRFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"fft_length\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}],\n    spectral = Object.freeze({\n  json: json$14\n}),\n    json$15 = [{\n  tfOpName: \"Cast\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"SrcT\",\n    name: \"sdtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"DstT\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"ExpandDims\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Pad\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"padding\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"constant_value\",\n    name: \"constantValue\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"PadV2\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"padding\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"constantValue\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Reshape\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"shape\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Squeeze\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    tfDeprecatedName: \"squeeze_dims\",\n    name: \"axis\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"SpaceToBatchND\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"blockShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"paddings\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"BatchToSpaceND\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"blockShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"crops\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"DepthToSpace\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"block_size\",\n    name: \"blockSize\",\n    type: \"number\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\"\n  }]\n}],\n    transformation = Object.freeze({\n  json: json$15\n}),\n    OperationMapper = function () {\n  function e() {\n    var e = [arithmetic, basicMath, control, convolution, creation, dynamic, evaluation, logical, image$1, graph, matrices, normalization, reduction, sliceJoin, spectral, transformation],\n        t = [].concat.apply([], e.map(function (e) {\n      return e.json;\n    }));\n    this.opMappers = t.reduce(function (e, t) {\n      return e[t.tfOpName] = t, e;\n    }, {});\n  }\n\n  return Object.defineProperty(e, \"Instance\", {\n    get: function get() {\n      return this._instance || (this._instance = new this());\n    },\n    enumerable: !0,\n    configurable: !0\n  }), e.prototype.transformGraph = function (e) {\n    var t = this,\n        a = [],\n        r = [],\n        n = e.node.reduce(function (e, n) {\n      return e[n.name] = t.mapNode(n), \"Placeholder\" === n.op && a.push(e[n.name]), \"Const\" === n.op && r.push(e[n.name]), e;\n    }, {}),\n        s = [],\n        o = [],\n        p = Object.keys(n);\n    return p.forEach(function (e) {\n      var t = n[e];\n      t.inputNames.forEach(function (e) {\n        var a = getNodeNameAndIndex(e)[0];\n        t.inputs.push(n[a]), n[a].children.push(t);\n      }), 0 === t.inputs.length && s.push(t);\n    }), p.forEach(function (e) {\n      var t = n[e];\n      0 === t.children.length && o.push(t);\n    }), {\n      nodes: n,\n      inputs: s,\n      outputs: o,\n      weights: r,\n      placeholders: a\n    };\n  }, e.prototype.mapNode = function (e) {\n    var t = getRegisteredOp(e.op) || this.opMappers[e.op] || {};\n    null == e.attr && (e.attr = {});\n    var a = {\n      name: e.name,\n      op: e.op,\n      category: t.category,\n      inputNames: (e.input || []).map(function (e) {\n        return e.startsWith(\"^\") ? e.substr(1) : e;\n      }),\n      inputs: [],\n      children: [],\n      inputParams: {},\n      attrParams: {},\n      rawAttrs: e.attr\n    };\n    return null != t.inputs && (a.inputParams = t.inputs.reduce(function (e, t) {\n      return e[t.name] = {\n        type: t.type,\n        inputIndexStart: t.start,\n        inputIndexEnd: t.end\n      }, e;\n    }, {})), null != t.attrs && (a.attrParams = t.attrs.reduce(function (t, a) {\n      var r = a.type,\n          n = void 0;\n\n      switch (a.type) {\n        case \"string\":\n          void 0 === (n = getStringParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getStringParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"string[]\":\n          void 0 === (n = getStringArrayParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getStringArrayParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"number\":\n          void 0 === (n = getNumberParam(e.attr, a.tfName, a.defaultValue || 0)) && a.tfDeprecatedName && (n = getNumberParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"number[]\":\n          void 0 === (n = getNumericArrayParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getNumericArrayParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"bool\":\n          void 0 === (n = getBoolParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getBoolParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"bool[]\":\n          void 0 === (n = getBoolArrayParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getBoolArrayParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"shape\":\n          void 0 === (n = getTensorShapeParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getTensorShapeParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"shape[]\":\n          void 0 === (n = getTensorShapeArrayParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getTensorShapeArrayParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"dtype\":\n          void 0 === (n = getDtypeParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getDtypeParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"dtype[]\":\n          void 0 === (n = getDtypeArrayParam(e.attr, a.tfName, a.defaultValue)) && a.tfDeprecatedName && (n = getDtypeArrayParam(e.attr, a.tfDeprecatedName, a.defaultValue));\n          break;\n\n        case \"tensor\":\n        case \"tensors\":\n          break;\n\n        default:\n          throw new Error(\"Unsupported param type: \" + a.type + \" for op: \" + e.op);\n      }\n\n      return t[a.name] = {\n        value: n,\n        type: r\n      }, t;\n    }, {})), a;\n  }, e;\n}();\n\nfunction decodeBase64(e) {\n  var t = env().global;\n  if (void 0 !== t.atob) return t.atob(e);\n  if (\"undefined\" != typeof Buffer) return new Buffer(e, \"base64\").toString();\n  throw new Error(\"Unable to decode base64 in this environment. Missing built-in atob() or Buffer()\");\n}\n\nfunction parseStringParam(e, t) {\n  var a = Array.isArray(e) ? String.fromCharCode.apply(null, e) : decodeBase64(e);\n  return t ? a : a.toLowerCase();\n}\n\nfunction getStringParam(e, t, a, r) {\n  void 0 === r && (r = !1);\n  var n = e[t];\n  return null != n ? parseStringParam(n.s, r) : a;\n}\n\nfunction getBoolParam(e, t, a) {\n  var r = e[t];\n  return r ? r.b : a;\n}\n\nfunction getNumberParam(e, t, a) {\n  var r = e[t] || {},\n      n = null != r.i ? r.i : null != r.f ? r.f : a;\n  return \"number\" == typeof n ? n : parseInt(n, 10);\n}\n\nfunction parseDtypeParam(e) {\n  switch (\"string\" == typeof e && (e = DataType[e]), e) {\n    case DataType.DT_FLOAT:\n      return \"float32\";\n\n    case DataType.DT_INT32:\n    case DataType.DT_INT64:\n      return \"int32\";\n\n    case DataType.DT_BOOL:\n      return \"bool\";\n\n    case DataType.DT_DOUBLE:\n      return \"float32\";\n\n    case DataType.DT_STRING:\n      return \"string\";\n\n    default:\n      return null;\n  }\n}\n\nfunction getDtypeParam(e, t, a) {\n  var r = e[t];\n  return r && r.type ? parseDtypeParam(r.type) : a;\n}\n\nfunction getDtypeArrayParam(e, t, a) {\n  var r = e[t];\n  return r && r.list && r.list.type ? r.list.type.map(function (e) {\n    return parseDtypeParam(e);\n  }) : a;\n}\n\nfunction parseTensorShapeParam(e) {\n  if (!e.unknownRank) return null != e.dim ? e.dim.map(function (e) {\n    return \"number\" == typeof e.size ? e.size : parseInt(e.size, 10);\n  }) : [];\n}\n\nfunction getTensorShapeParam(e, t, a) {\n  var r = e[t];\n  return r && r.shape ? parseTensorShapeParam(r.shape) : a;\n}\n\nfunction getNumericArrayParam(e, t, a) {\n  var r = e[t];\n  return r ? ((r.list.f && r.list.f.length ? r.list.f : r.list.i) || []).map(function (e) {\n    return \"number\" == typeof e ? e : parseInt(e, 10);\n  }) : a;\n}\n\nfunction getStringArrayParam(e, t, a, r) {\n  void 0 === r && (r = !1);\n  var n = e[t];\n  return n && n.list && n.list.s ? n.list.s.map(function (e) {\n    return parseStringParam(e, r);\n  }) : a;\n}\n\nfunction getTensorShapeArrayParam(e, t, a) {\n  var r = e[t];\n  return r && r.list && r.list.shape ? r.list.shape.map(function (e) {\n    return parseTensorShapeParam(e);\n  }) : a;\n}\n\nfunction getBoolArrayParam(e, t, a) {\n  var r = e[t];\n  return r && r.list && r.list.b ? r.list.b : a;\n}\n\nvar NodeValueImpl = function () {\n  function e(e, t, a) {\n    var r = this;\n    this.node = e, this.tensorMap = t, this.context = a, this.inputs = [], this.attrs = {}, this.inputs = e.inputNames.map(function (e) {\n      return r.getInput(e);\n    }), null != e.rawAttrs && (this.attrs = Object.keys(e.rawAttrs).reduce(function (e, t) {\n      return e[t] = r.getAttr(t), e;\n    }, {}));\n  }\n\n  return e.prototype.getInput = function (e) {\n    return getTensor(e, this.tensorMap, this.context);\n  }, e.prototype.getAttr = function (e, t) {\n    var a = this.node.rawAttrs[e];\n    if (null != a.tensor) return getTensor(e, this.tensorMap, this.context);\n    if (null != a.i || null != a.f) return getNumberParam(this.node.rawAttrs, e, t);\n    if (null != a.s) return getStringParam(this.node.rawAttrs, e, t);\n    if (null != a.b) return getBoolParam(this.node.rawAttrs, e, t);\n    if (null != a.shape) return getTensorShapeParam(this.node.rawAttrs, e, t);\n    if (null != a.type) return getDtypeParam(this.node.rawAttrs, e, t);\n\n    if (null != a.list) {\n      if (null != a.list.i || null != a.list.f) return getNumericArrayParam(this.node.rawAttrs, e, t);\n      if (null != a.list.s) return getStringArrayParam(this.node.rawAttrs, e, t);\n      if (null != a.list.shape) return getTensorShapeArrayParam(this.node.rawAttrs, e, t);\n      if (null != a.list.b) return getBoolArrayParam(this.node.rawAttrs, e, t);\n      if (null != a.list.type) return getDtypeArrayParam(this.node.rawAttrs, e, t);\n    }\n\n    return t;\n  }, e;\n}(),\n    executeOp = function executeOp(e, t, a) {\n  switch (e.op) {\n    case \"BiasAdd\":\n    case \"AddV2\":\n    case \"Add\":\n      return [add(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"AddN\":\n      return [addN(getParamValue(\"tensors\", e, t, a))];\n\n    case \"FloorMod\":\n    case \"Mod\":\n      return [mod(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Mul\":\n      return [mul(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"RealDiv\":\n    case \"Div\":\n      return [div(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"FloorDiv\":\n      return [floorDiv(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Sub\":\n      return [sub(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Minimum\":\n      return [minimum(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Maximum\":\n      return [maximum(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Pow\":\n      return [pow(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"SquaredDifference\":\n      return [squaredDifference(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$1 = function executeOp$1(e, t, a) {\n  switch (e.op) {\n    case \"Abs\":\n    case \"ComplexAbs\":\n      return [abs(getParamValue(\"x\", e, t, a))];\n\n    case \"Acos\":\n      return [acos(getParamValue(\"x\", e, t, a))];\n\n    case \"Acosh\":\n      return [acosh(getParamValue(\"x\", e, t, a))];\n\n    case \"Asin\":\n      return [asin(getParamValue(\"x\", e, t, a))];\n\n    case \"Asinh\":\n      return [asinh(getParamValue(\"x\", e, t, a))];\n\n    case \"Atan\":\n      return [atan(getParamValue(\"x\", e, t, a))];\n\n    case \"Atan2\":\n      return [atan2(getParamValue(\"x\", e, t, a), getParamValue(\"y\", e, t, a))];\n\n    case \"Atanh\":\n      return [atanh(getParamValue(\"x\", e, t, a))];\n\n    case \"Ceil\":\n      return [ceil(getParamValue(\"x\", e, t, a))];\n\n    case \"Complex\":\n      return [complex(getParamValue(\"real\", e, t, a), getParamValue(\"imag\", e, t, a))];\n\n    case \"Cos\":\n      return [cos(getParamValue(\"x\", e, t, a))];\n\n    case \"Cosh\":\n      return [cosh(getParamValue(\"x\", e, t, a))];\n\n    case \"Elu\":\n      return [elu(getParamValue(\"x\", e, t, a))];\n\n    case \"Erf\":\n      return [erf(getParamValue(\"x\", e, t, a))];\n\n    case \"Exp\":\n      return [exp(getParamValue(\"x\", e, t, a))];\n\n    case \"Expm1\":\n      return [expm1(getParamValue(\"x\", e, t, a))];\n\n    case \"Floor\":\n      return [floor(getParamValue(\"x\", e, t, a))];\n\n    case \"Log\":\n      return [log(getParamValue(\"x\", e, t, a))];\n\n    case \"Log1p\":\n      return [log1p(getParamValue(\"x\", e, t, a))];\n\n    case \"Imag\":\n      return [imag(getParamValue(\"x\", e, t, a))];\n\n    case \"Neg\":\n      return [neg(getParamValue(\"x\", e, t, a))];\n\n    case \"Reciprocal\":\n      return [reciprocal(getParamValue(\"x\", e, t, a))];\n\n    case \"Real\":\n      return [real(getParamValue(\"x\", e, t, a))];\n\n    case \"Relu\":\n      return [relu(getParamValue(\"x\", e, t, a))];\n\n    case \"Round\":\n      return [round(getParamValue(\"x\", e, t, a))];\n\n    case \"Selu\":\n      return [selu(getParamValue(\"x\", e, t, a))];\n\n    case \"Sigmoid\":\n      return [sigmoid(getParamValue(\"x\", e, t, a))];\n\n    case \"Sin\":\n      return [sin(getParamValue(\"x\", e, t, a))];\n\n    case \"Sign\":\n      return [sign(getParamValue(\"x\", e, t, a))];\n\n    case \"Sinh\":\n      return [sinh(getParamValue(\"x\", e, t, a))];\n\n    case \"Softplus\":\n      return [softplus(getParamValue(\"x\", e, t, a))];\n\n    case \"Sqrt\":\n      return [sqrt(getParamValue(\"x\", e, t, a))];\n\n    case \"Square\":\n      return [square(getParamValue(\"x\", e, t, a))];\n\n    case \"Tanh\":\n      return [tanh(getParamValue(\"x\", e, t, a))];\n\n    case \"Tan\":\n      return [tan(getParamValue(\"x\", e, t, a))];\n\n    case \"Relu6\":\n    case \"ClipByValue\":\n      return [clipByValue(getParamValue(\"x\", e, t, a), getParamValue(\"clipValueMin\", e, t, a), getParamValue(\"clipValueMax\", e, t, a))];\n\n    case \"Rsqrt\":\n      return [rsqrt(getTensor(e.inputNames[0], t, a))];\n\n    case \"Prod\":\n      return [prod(getParamValue(\"x\", e, t, a), getParamValue(\"axes\", e, t, a))];\n\n    case \"LeakyRelu\":\n      return [leakyRelu(getParamValue(\"x\", e, t, a), getParamValue(\"alpha\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    TensorArray = function () {\n  function e(t, a, r, n, s, o, p) {\n    this.name = t, this.dtype = a, this.maxSize = r, this.elementShape = n, this.identicalElementShapes = s, this.dynamicSize = o, this.clearAfterRead = p, this.tensors = [], this.closed_ = !1, this.id = e.nextId++;\n  }\n\n  return Object.defineProperty(e.prototype, \"closed\", {\n    get: function get() {\n      return this.closed_;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), e.prototype.clearAndClose = function () {\n    this.tensors.forEach(function (e) {\n      return e.tensor.dispose();\n    }), this.tensors = [], this.closed_ = !0;\n  }, e.prototype.size = function () {\n    return this.tensors.length;\n  }, e.prototype.read = function (e) {\n    if (this.closed_) throw new Error(\"TensorArray \" + this.name + \" has already been closed.\");\n    if (e < 0 || e >= this.tensors.length) throw new Error(\"Tried to read from index \" + e + \", but array size is: \" + this.tensors.length);\n    var t = this.tensors[e];\n    if (t.cleared) throw new Error(\"TensorArray \" + this.name + \": Could not read index \" + e + \" twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?).\");\n    return this.clearAfterRead && (t.cleared = !0), t.read = !0, t.tensor;\n  }, e.prototype.readMany = function (e) {\n    var t = this;\n    return e.map(function (e) {\n      return t.read(e);\n    });\n  }, e.prototype.write = function (e, t) {\n    if (this.closed_) throw new Error(\"TensorArray \" + this.name + \" has already been closed.\");\n    if (e < 0 || !this.dynamicSize && e >= this.maxSize) throw new Error(\"Tried to write to index \" + e + \", but array is not resizeable and size is: \" + this.maxSize);\n    var a = this.tensors[e] || {};\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray \" + this.name + \": Could not write to TensorArray index \" + e + \",\\n          because the value dtype is \" + t.dtype + \", but TensorArray dtype is \" + this.dtype + \".\");\n    if (0 !== this.size() || null != this.elementShape && 0 !== this.elementShape.length || (this.elementShape = t.shape), this.assertShapesMatchAllowUndefinedSize(this.elementShape, t.shape, \"TensorArray \" + this.name + \": Could not write to TensorArray index \" + e + \".\"), a && a.read) throw new Error(\"TensorArray \" + this.name + \": Could not write to TensorArray index \" + e + \", because it has already been read.\");\n    if (a && a.written) throw new Error(\"TensorArray \" + this.name + \": Could not write to TensorArray index \" + e + \", because it has already been written.\");\n    a.tensor = t, a.written = !0, this.tensors[e] = a;\n  }, e.prototype.writeMany = function (e, t) {\n    var a = this;\n    if (e.length !== t.length) throw new Error(\"TensorArray \" + this.name + \": could not write multiple tensors,because the index size: \" + e.length + \" is not the same as tensors size: \" + t.length + \".\");\n    e.forEach(function (e, r) {\n      return a.write(e, t[r]);\n    });\n  }, e.prototype.gather = function (e, t) {\n    if (t && t !== this.dtype) throw new Error(\"TensorArray dtype is \" + this.dtype + \" but gather requested dtype \" + t);\n\n    if (!e) {\n      e = [];\n\n      for (var a = 0; a < this.size(); a++) {\n        e.push(a);\n      }\n    }\n\n    if (0 === e.length) return tensor([], [0].concat(this.elementShape));\n    var r = this.readMany(e);\n    return this.assertShapesMatchAllowUndefinedSize(this.elementShape, r[0].shape, \"TensorArray shape mismatch: \"), stack(r, 0);\n  }, e.prototype.concat = function (e) {\n    if (e && e !== this.dtype) throw new Error(\"TensorArray dtype is \" + this.dtype + \" but concat requested dtype \" + e);\n    if (0 === this.size()) return tensor([], [0].concat(this.elementShape));\n\n    for (var t = [], a = 0; a < this.size(); a++) {\n      t.push(a);\n    }\n\n    var r = this.readMany(t);\n    return this.assertShapesMatchAllowUndefinedSize(this.elementShape, r[0].shape, \"TensorArray shape mismatch: tensor array shape (\" + this.elementShape + \") vs first tensor shape (\" + r[0].shape + \")\"), concat(r, 0);\n  }, e.prototype.scatter = function (e, t) {\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray dtype is \" + this.dtype + \" but tensor has dtype \" + t.dtype);\n    if (e.length !== t.shape[0]) throw new Error(\"Expected len(indices) == tensor.shape[0], but saw: \" + e.length + \" vs. \" + t.shape[0]);\n    var a = Math.max.apply(Math, e);\n    if (!this.dynamicSize && a >= this.maxSize) throw new Error(\"Max index must be < array size (\" + a + \"  vs. \" + this.maxSize + \")\");\n    this.writeMany(e, unstack(t, 0));\n  }, e.prototype.split = function (e, t) {\n    var a = this;\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray dtype is \" + this.dtype + \" but tensor has dtype \" + t.dtype);\n    var r = 0,\n        n = e.map(function (e) {\n      return r += e;\n    });\n    if (r !== t.shape[0]) throw new Error(\"Expected sum of lengths to be equal to\\n          tensor.shape[0], but sum of lengths is\\n        \" + r + \", and tensor's shape is: \" + t.shape);\n    if (!this.dynamicSize && e.length !== this.maxSize) throw new Error(\"TensorArray's size is not equal to the size of lengths (\" + this.maxSize + \" vs. \" + e.length + \"), and the TensorArray is not marked as dynamically resizeable\");\n    var s = 0 === r ? 0 : t.size / r,\n        o = [];\n    tidy(function () {\n      t = t.reshape([1, r, s]);\n\n      for (var p = 0; p < e.length; ++p) {\n        var u = [0, 0 === p ? 0 : n[p - 1], 0],\n            i = [1, e[p], s];\n        o[p] = slice(t, u, i).reshape(a.elementShape);\n      }\n\n      return o;\n    });\n\n    for (var p = [], u = 0; u < e.length; u++) {\n      p[u] = u;\n    }\n\n    this.writeMany(p, o);\n  }, e.prototype.assertShapesMatchAllowUndefinedSize = function (e, t, a) {\n    void 0 === a && (a = \"\"), util.assert(this.shapesEqualAllowUndefinedSize(e, t), function () {\n      return a + \" Shapes \" + e + \" and \" + t + \" must match\";\n    });\n  }, e.prototype.shapesEqualAllowUndefinedSize = function (e, t) {\n    if (e.length !== t.length) return !1;\n\n    for (var a = 0; a < e.length; a++) {\n      if (-1 !== e[a] && -1 !== t[a] && e[a] !== t[a]) return !1;\n    }\n\n    return !0;\n  }, e.nextId = 0, e;\n}();\n\nfunction executeOp$2(e, t, a) {\n  return __awaiter(this, void 0, void 0, function () {\n    var r, n, s, o, p, u, i, m, l, c, d, y, f, g, h, N, x, V, P, b, T, O, S, v, _, w, A, D, E, I, C, M, k, z, j;\n\n    return __generator(this, function (F) {\n      switch (F.label) {\n        case 0:\n          switch (e.op) {\n            case \"LoopCond\":\n              return [3, 1];\n\n            case \"Switch\":\n              return [3, 2];\n\n            case \"Merge\":\n              return [3, 4];\n\n            case \"Enter\":\n              return [3, 5];\n\n            case \"Exit\":\n              return [3, 6];\n\n            case \"NextIteration\":\n              return [3, 7];\n\n            case \"TensorArrayV3\":\n              return [3, 8];\n\n            case \"TensorArrayWriteV3\":\n              return [3, 9];\n\n            case \"TensorArrayReadV3\":\n              return [3, 10];\n\n            case \"TensorArrayGatherV3\":\n              return [3, 11];\n\n            case \"TensorArrayScatterV3\":\n              return [3, 12];\n\n            case \"TensorArrayConcatV3\":\n              return [3, 13];\n\n            case \"TensorArraySplitV3\":\n              return [3, 14];\n\n            case \"TensorArraySizeV3\":\n              return [3, 15];\n\n            case \"TensorArrayCloseV3\":\n              return [3, 16];\n          }\n\n          return [3, 17];\n\n        case 1:\n          return [2, [getParamValue(\"pred\", e, t, a).clone()]];\n\n        case 2:\n          return r = getParamValue(\"pred\", e, t, a), n = getParamValue(\"data\", e, t, a), [4, r.data()];\n\n        case 3:\n          return [2, F.sent()[0] ? [void 0, n.clone()] : [n.clone(), void 0]];\n\n        case 4:\n          return [2, (s = e.inputNames.find(function (e) {\n            return void 0 !== getTensor(e, t, a);\n          })) ? [getTensor(s, t, a).clone()] : void 0];\n\n        case 5:\n          return o = getParamValue(\"frameName\", e, t, a), p = getParamValue(\"tensor\", e, t, a), a.enterFrame(o), [2, [p.clone()]];\n\n        case 6:\n          return u = getParamValue(\"tensor\", e, t, a), a.exitFrame(), [2, [u.clone()]];\n\n        case 7:\n          return i = getParamValue(\"tensor\", e, t, a), a.nextIteration(), [2, [i.clone()]];\n\n        case 8:\n          return m = getParamValue(\"size\", e, t, a), l = getParamValue(\"dtype\", e, t, a), c = getParamValue(\"elementShape\", e, t, a), d = getParamValue(\"dynamicSize\", e, t, a), y = getParamValue(\"clearAfterRead\", e, t, a), f = getParamValue(\"identicalElementShapes\", e, t, a), g = getParamValue(\"name\", e, t, a), h = new TensorArray(g, l, m, c, f, d, y), a.addTensorArray(h), [2, [scalar(h.id), scalar(1)]];\n\n        case 9:\n          return N = getParamValue(\"tensorArrayId\", e, t, a), x = getParamValue(\"index\", e, t, a), V = getParamValue(\"tensor\", e, t, a), a.getTensorArray(N).write(x, V), [2, [scalar(1)]];\n\n        case 10:\n          return P = getParamValue(\"tensorArrayId\", e, t, a), b = getParamValue(\"index\", e, t, a), [2, [a.getTensorArray(P).read(b)]];\n\n        case 11:\n          return T = getParamValue(\"tensorArrayId\", e, t, a), O = getParamValue(\"indices\", e, t, a), S = getParamValue(\"dtype\", e, t, a), [2, [a.getTensorArray(T).gather(O, S)]];\n\n        case 12:\n          return v = getParamValue(\"tensorArrayId\", e, t, a), _ = getParamValue(\"indices\", e, t, a), w = getParamValue(\"tensor\", e, t, a), a.getTensorArray(v).scatter(_, w), [2, [scalar(1)]];\n\n        case 13:\n          return A = getParamValue(\"tensorArrayId\", e, t, a), D = a.getTensorArray(A), E = getParamValue(\"dtype\", e, t, a), [2, [D.concat(E)]];\n\n        case 14:\n          return I = getParamValue(\"tensorArrayId\", e, t, a), C = getParamValue(\"tensor\", e, t, a), M = getParamValue(\"lengths\", e, t, a), a.getTensorArray(I).split(M, C), [2, [scalar(1)]];\n\n        case 15:\n          return k = getParamValue(\"tensorArrayId\", e, t, a), z = a.getTensorArray(k), [2, [scalar(z.size(), \"int32\")]];\n\n        case 16:\n          return j = getParamValue(\"tensorArrayId\", e, t, a), a.getTensorArray(j).clearAndClose(), [2, []];\n\n        case 17:\n          throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n      }\n    });\n  });\n}\n\nvar executeOp$3 = function executeOp$3(e, t, a) {\n  var r, n;\n\n  switch (e.op) {\n    case \"Conv1D\":\n      var s = getParamValue(\"stride\", e, t, a),\n          o = getParamValue(\"pad\", e, t, a),\n          p = getParamValue(\"dataFormat\", e, t, a).toUpperCase(),\n          u = getParamValue(\"dilation\", e, t, a);\n      return [conv1d(getParamValue(\"x\", e, t, a), getParamValue(\"filter\", e, t, a), s, o, p, u)];\n\n    case \"Conv2D\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), p = getParamValue(\"dataFormat\", e, t, a).toUpperCase();\n      var i = getParamValue(\"dilations\", e, t, a);\n      return [conv2d(getParamValue(\"x\", e, t, a), getParamValue(\"filter\", e, t, a), [s[1], s[2]], o, p, [i[1], i[2]])];\n\n    case \"_FusedConv2D\":\n      var m = (r = getParamValue(\"fusedOps\", e, t, a))[0],\n          l = r[1],\n          c = \"biasadd\" === m,\n          d = \"prelu\" === l,\n          y = \"fusedbatchnorm\" === m,\n          f = getParamValue(\"numArgs\", e, t, a);\n\n      if (c) {\n        if (d && 2 !== f) throw new Error(\"Fused Conv2d with BiasAdd and Prelu must have two extra arguments: bias and alpha.\");\n        if (!d && 1 !== f) throw new Error(\"Fused Conv2d with BiasAdd must have one extra argument: bias.\");\n      }\n\n      if (y) throw new Error(\"Fused Conv2d with FusedBatchNorm is not supported.\");\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), p = getParamValue(\"dataFormat\", e, t, a).toUpperCase(), i = getParamValue(\"dilations\", e, t, a);\n      var g = (n = getParamValue(\"args\", e, t, a))[0],\n          h = n[1];\n      return [fused.conv2d({\n        x: getParamValue(\"x\", e, t, a),\n        filter: getParamValue(\"filter\", e, t, a),\n        strides: [s[1], s[2]],\n        pad: o,\n        dataFormat: p,\n        dilations: [i[1], i[2]],\n        bias: g,\n        activation: l,\n        preluActivationWeights: h\n      })];\n\n    case \"Conv2DBackpropInput\":\n    case \"Conv2dTranspose\":\n      var N = getParamValue(\"outputShape\", e, t, a);\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a);\n      return [conv2dTranspose(getParamValue(\"x\", e, t, a), getParamValue(\"filter\", e, t, a), N, [s[1], s[2]], o)];\n\n    case \"DepthwiseConv2dNative\":\n    case \"DepthwiseConv2d\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), i = getParamValue(\"dilations\", e, t, a), p = getParamValue(\"dataFormat\", e, t, a).toUpperCase();\n      return [depthwiseConv2d(getParamValue(\"input\", e, t, a), getParamValue(\"filter\", e, t, a), [s[1], s[2]], o, p, [i[1], i[2]])];\n\n    case \"Conv3D\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), p = getParamValue(\"dataFormat\", e, t, a).toUpperCase(), i = getParamValue(\"dilations\", e, t, a);\n      return [conv3d(getParamValue(\"x\", e, t, a), getParamValue(\"filter\", e, t, a), [s[1], s[2], s[3]], o, p, [i[1], i[2], i[3]])];\n\n    case \"AvgPool\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a);\n      var x = getParamValue(\"kernelSize\", e, t, a);\n      return [avgPool(getParamValue(\"x\", e, t, a), [x[1], x[2]], [s[1], s[2]], o)];\n\n    case \"MaxPool\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), x = getParamValue(\"kernelSize\", e, t, a);\n      return [maxPool(getParamValue(\"x\", e, t, a), [x[1], x[2]], [s[1], s[2]], o)];\n\n    case \"AvgPool3D\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), x = getParamValue(\"kernelSize\", e, t, a);\n      return [avgPool3d(getParamValue(\"x\", e, t, a), [x[1], x[2], x[3]], [s[1], s[2], s[3]], o)];\n\n    case \"MaxPool3D\":\n      s = getParamValue(\"strides\", e, t, a), o = getParamValue(\"pad\", e, t, a), x = getParamValue(\"kernelSize\", e, t, a);\n      return [maxPool3d(getParamValue(\"x\", e, t, a), [x[1], x[2], x[3]], [s[1], s[2], s[3]], o)];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$4 = function executeOp$4(e, t, a) {\n  switch (e.op) {\n    case \"Fill\":\n      var r = getParamValue(\"shape\", e, t, a),\n          n = getParamValue(\"dtype\", e, t, a),\n          s = getParamValue(\"value\", e, t, a);\n      return [fill(r, s, n)];\n\n    case \"LinSpace\":\n      var o = getParamValue(\"start\", e, t, a),\n          p = getParamValue(\"stop\", e, t, a),\n          u = getParamValue(\"num\", e, t, a);\n      return [linspace(o, p, u)];\n\n    case \"Multinomial\":\n      var i = getParamValue(\"logits\", e, t, a),\n          m = getParamValue(\"numSamples\", e, t, a),\n          l = getParamValue(\"seed\", e, t, a);\n      return [multinomial(i, m, l)];\n\n    case \"OneHot\":\n      var c = getParamValue(\"indices\", e, t, a),\n          d = getParamValue(\"depth\", e, t, a),\n          y = getParamValue(\"onValue\", e, t, a),\n          f = getParamValue(\"offValue\", e, t, a);\n      return [oneHot(c, d, y, f)];\n\n    case \"Ones\":\n      return [ones(getParamValue(\"shape\", e, t, a), getParamValue(\"dtype\", e, t, a))];\n\n    case \"OnesLike\":\n      return [onesLike(getParamValue(\"x\", e, t, a))];\n\n    case \"RandomUniform\":\n      return [randomUniform(getParamValue(\"shape\", e, t, a), getParamValue(\"minval\", e, t, a), getParamValue(\"maxval\", e, t, a), getParamValue(\"dtype\", e, t, a))];\n\n    case \"Range\":\n      o = getParamValue(\"start\", e, t, a);\n      var g = getParamValue(\"stop\", e, t, a),\n          h = getParamValue(\"step\", e, t, a);\n      return [range(o, g, h, getParamValue(\"dtype\", e, t, a))];\n\n    case \"TruncatedNormal\":\n      r = getParamValue(\"shape\", e, t, a);\n      var N = getParamValue(\"mean\", e, t, a),\n          x = getParamValue(\"stdDev\", e, t, a);\n      l = getParamValue(\"seed\", e, t, a);\n      return [truncatedNormal(r, N, x, getParamValue(\"dtype\", e, t, a), l)];\n\n    case \"Zeros\":\n      return [zeros(getParamValue(\"shape\", e, t, a), getParamValue(\"dtype\", e, t, a))];\n\n    case \"ZerosLike\":\n      return [zerosLike(getParamValue(\"x\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n};\n\nfunction executeOp$5(e, t, a) {\n  return __awaiter(this, void 0, void 0, function () {\n    var r, n, s, o, p;\n    return __generator(this, function (u) {\n      switch (u.label) {\n        case 0:\n          switch (e.op) {\n            case \"NonMaxSuppressionV3\":\n            case \"NonMaxSuppressionV2\":\n              return [3, 1];\n\n            case \"Where\":\n              return [3, 3];\n\n            case \"ListDiff\":\n              return [3, 5];\n          }\n\n          return [3, 6];\n\n        case 1:\n          return r = getParamValue(\"boxes\", e, t, a), n = getParamValue(\"scores\", e, t, a), s = getParamValue(\"maxOutputSize\", e, t, a), o = getParamValue(\"iouThreshold\", e, t, a), p = getParamValue(\"scoreThreshold\", e, t, a), [4, image.nonMaxSuppressionAsync(r, n, s, o, p)];\n\n        case 2:\n          return [2, [u.sent()]];\n\n        case 3:\n          return [4, whereAsync(getParamValue(\"condition\", e, t, a).asType(\"bool\"))];\n\n        case 4:\n          return [2, [u.sent()]];\n\n        case 5:\n          return [2, setdiff1dAsync(getParamValue(\"x\", e, t, a), getParamValue(\"y\", e, t, a))];\n\n        case 6:\n          throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n      }\n    });\n  });\n}\n\nvar executeOp$6 = function executeOp$6(e, t, a) {\n  switch (e.op) {\n    case \"TopKV2\":\n      var r = getParamValue(\"x\", e, t, a),\n          n = getParamValue(\"k\", e, t, a),\n          s = getParamValue(\"sorted\", e, t, a),\n          o = topk(r, n, s);\n      return [o.values, o.indices];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$7 = function executeOp$7(e, t, a) {\n  switch (e.op) {\n    case \"Const\":\n      return t[e.name];\n\n    case \"PlaceholderWithDefault\":\n      var r = getParamValue(\"default\", e, t, a);\n      return [getTensor(e.name, t, a) || r];\n\n    case \"Placeholder\":\n      return [getTensor(e.name, t, a)];\n\n    case \"Identity\":\n    case \"StopGradient\":\n    case \"FakeQuantWithMinMaxVars\":\n      return [getParamValue(\"x\", e, t, a).clone()];\n\n    case \"IdentityN\":\n      return getParamValue(\"x\", e, t, a).map(function (e) {\n        return e.clone();\n      });\n\n    case \"Snapshot\":\n      return [getParamValue(\"x\", e, t, a).clone()];\n\n    case \"Shape\":\n      return [tensor1d(getParamValue(\"x\", e, t, a).shape, \"int32\")];\n\n    case \"ShapeN\":\n      return getParamValue(\"x\", e, t, a).map(function (e) {\n        return tensor1d(e.shape);\n      });\n\n    case \"Size\":\n      return [scalar(getParamValue(\"x\", e, t, a).size, \"int32\")];\n\n    case \"Rank\":\n      return [scalar(getParamValue(\"x\", e, t, a).rank, \"int32\")];\n\n    case \"NoOp\":\n      return [];\n\n    case \"Print\":\n      var n = getParamValue(\"x\", e, t, a),\n          s = getParamValue(\"data\", e, t, a),\n          o = getParamValue(\"message\", e, t, a),\n          p = getParamValue(\"summarize\", e, t, a);\n      console.warn(\"The graph has a tf.print() operation,usually used for debugging, which slows down performance.\"), console.log(o);\n\n      for (var u = 0; u < s.length; u++) {\n        console.log(Array.prototype.slice.call(s[u].dataSync()).slice(0, p));\n      }\n\n      return [n];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$8 = function executeOp$8(e, t, a) {\n  switch (e.op) {\n    case \"ResizeBilinear\":\n      var r = getParamValue(\"images\", e, t, a),\n          n = getParamValue(\"size\", e, t, a),\n          s = getParamValue(\"alignCorners\", e, t, a);\n      return [image.resizeBilinear(r, [n[0], n[1]], s)];\n\n    case \"ResizeNearestNeighbor\":\n      r = getParamValue(\"images\", e, t, a), n = getParamValue(\"size\", e, t, a), s = getParamValue(\"alignCorners\", e, t, a);\n      return [image.resizeNearestNeighbor(r, [n[0], n[1]], s)];\n\n    case \"CropAndResize\":\n      var o = getParamValue(\"image\", e, t, a),\n          p = getParamValue(\"boxes\", e, t, a),\n          u = getParamValue(\"boxInd\", e, t, a),\n          i = getParamValue(\"cropSize\", e, t, a),\n          m = getParamValue(\"method\", e, t, a),\n          l = getParamValue(\"extrapolationValue\", e, t, a);\n      return [image.cropAndResize(o, p, u, i, m, l)];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$9 = function executeOp$9(e, t, a) {\n  switch (e.op) {\n    case \"Equal\":\n      return [equal(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"NotEqual\":\n      return [notEqual(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Greater\":\n      return [greater(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"GreaterEqual\":\n      return [greaterEqual(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Less\":\n      return [less(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"LessEqual\":\n      return [lessEqual(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"LogicalAnd\":\n      return [logicalAnd(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"LogicalNot\":\n      return [logicalNot(getParamValue(\"a\", e, t, a))];\n\n    case \"LogicalOr\":\n      return [logicalOr(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    case \"Select\":\n      return [where(getParamValue(\"condition\", e, t, a), getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$10 = function executeOp$10(e, t, a) {\n  switch (e.op) {\n    case \"BatchMatMul\":\n    case \"BatchMatMulV2\":\n    case \"MatMul\":\n      return [matMul(getParamValue(\"a\", e, t, a), getParamValue(\"b\", e, t, a), getParamValue(\"transposeA\", e, t, a), getParamValue(\"transposeB\", e, t, a))];\n\n    case \"Transpose\":\n      return [transpose(getParamValue(\"x\", e, t, a), getParamValue(\"perm\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$11 = function executeOp$11(e, t, a) {\n  switch (e.op) {\n    case \"FusedBatchNorm\":\n    case \"FusedBatchNormV2\":\n    case \"FusedBatchNormV3\":\n      return [batchNorm(getParamValue(\"x\", e, t, a), getParamValue(\"mean\", e, t, a), getParamValue(\"variance\", e, t, a), getParamValue(\"offset\", e, t, a), getParamValue(\"scale\", e, t, a), getParamValue(\"epsilon\", e, t, a))];\n\n    case \"LRN\":\n      return [localResponseNormalization(getParamValue(\"x\", e, t, a), getParamValue(\"radius\", e, t, a), getParamValue(\"bias\", e, t, a), getParamValue(\"alpha\", e, t, a), getParamValue(\"beta\", e, t, a))];\n\n    case \"Softmax\":\n      return [softmax(getParamValue(\"x\", e, t, a))];\n\n    case \"LogSoftmax\":\n      return [logSoftmax(getParamValue(\"x\", e, t, a))];\n\n    case \"SparseToDense\":\n      return [sparseToDense(getParamValue(\"sparseIndices\", e, t, a), getParamValue(\"outputShape\", e, t, a), getParamValue(\"sparseValues\", e, t, a), getParamValue(\"defaultValue\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$12 = function executeOp$12(e, t, a) {\n  switch (e.op) {\n    case \"Max\":\n      var r = getParamValue(\"axis\", e, t, a),\n          n = getParamValue(\"keepDims\", e, t, a);\n      return [max(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"Mean\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [mean(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"Min\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [min(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"Sum\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [sum(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"All\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [all(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"Any\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [any(getParamValue(\"x\", e, t, a), r, n)];\n\n    case \"ArgMax\":\n      r = getParamValue(\"axis\", e, t, a);\n      return [argMax(getParamValue(\"x\", e, t, a), r)];\n\n    case \"ArgMin\":\n      r = getParamValue(\"axis\", e, t, a);\n      return [argMin(getParamValue(\"x\", e, t, a), r)];\n\n    case \"Prod\":\n      r = getParamValue(\"axis\", e, t, a), n = getParamValue(\"keepDims\", e, t, a);\n      return [prod(getParamValue(\"x\", e, t, a), r, n)];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$13 = function executeOp$13(e, t, a) {\n  switch (e.op) {\n    case \"ConcatV2\":\n    case \"Concat\":\n      var r = getParamValue(\"axis\", e, t, a),\n          n = getParamValue(\"tensors\", e, t, a);\n      return [concat(n, r)];\n\n    case \"GatherV2\":\n    case \"Gather\":\n      r = getParamValue(\"axis\", e, t, a);\n      var s = getParamValue(\"x\", e, t, a),\n          o = getParamValue(\"indices\", e, t, a);\n      return [gather(s, o.asType(\"int32\"), r)];\n\n    case \"ReverseV2\":\n    case \"Reverse\":\n      r = getParamValue(\"axis\", e, t, a), s = getParamValue(\"x\", e, t, a);\n      return [reverse(s, r)];\n\n    case \"Slice\":\n      var p = getParamValue(\"begin\", e, t, a),\n          u = getParamValue(\"size\", e, t, a);\n      return [slice(getParamValue(\"x\", e, t, a), p, u)];\n\n    case \"StridedSlice\":\n      p = getParamValue(\"begin\", e, t, a);\n      var i = getParamValue(\"end\", e, t, a),\n          m = getParamValue(\"strides\", e, t, a),\n          l = getParamValue(\"beginMask\", e, t, a),\n          c = getParamValue(\"endMask\", e, t, a),\n          d = getParamValue(\"ellipsisMask\", e, t, a),\n          y = getParamValue(\"newAxisMask\", e, t, a),\n          f = getParamValue(\"shrinkAxisMask\", e, t, a),\n          g = getParamValue(\"x\", e, t, a);\n      if (1 === p.length && g.shape.length > 1) for (var h = 1; h < g.shape.length; h++) {\n        p.push(0), i.push(g.shape[h]), m.push(m[0]);\n      }\n      return [stridedSlice(g, p, i, m, l, c, d, y, f)];\n\n    case \"Pack\":\n      return tidy(function () {\n        var r = getParamValue(\"axis\", e, t, a),\n            n = getParamValue(\"tensors\", e, t, a),\n            s = n[0].shape,\n            o = n[0].squeeze().shape,\n            p = n.map(function (e) {\n          var t = util.arraysEqual(e.shape, s);\n          if (!t && !util.arraysEqual(e.squeeze().shape, o)) throw new Error(\"the input tensors shape does not match\");\n          return t ? e : e.reshape(s);\n        });\n        return [stack(p, r)];\n      });\n\n    case \"Unpack\":\n      return tidy(function () {\n        var r = getParamValue(\"axis\", e, t, a),\n            n = getParamValue(\"tensor\", e, t, a);\n        return unstack(n, r);\n      });\n\n    case \"Tile\":\n      var N = getParamValue(\"reps\", e, t, a);\n      return [tile(getParamValue(\"x\", e, t, a), N)];\n\n    case \"Split\":\n    case \"SplitV\":\n      r = getParamValue(\"axis\", e, t, a);\n      var x = getParamValue(\"numOrSizeSplits\", e, t, a);\n      return split(getParamValue(\"x\", e, t, a), x, r);\n\n    case \"ScatterNd\":\n      o = getParamValue(\"indices\", e, t, a);\n      var V = getParamValue(\"values\", e, t, a),\n          P = getParamValue(\"shape\", e, t, a);\n      return [scatterND(o, V, P)];\n\n    case \"GatherNd\":\n      var b = getParamValue(\"x\", e, t, a);\n      o = getParamValue(\"indices\", e, t, a);\n      return [gatherND(b, o)];\n\n    case \"SparseToDense\":\n      o = getParamValue(\"sparseIndices\", e, t, a), P = getParamValue(\"outputShape\", e, t, a);\n      var T = getParamValue(\"sparseValues\", e, t, a),\n          O = getParamValue(\"defaultValue\", e, t, a);\n      return [sparseToDense(o, T, P, T.dtype === O.dtype ? O : O.asType(T.dtype))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$14 = function executeOp$14(e, t, a) {\n  switch (e.op) {\n    case \"FFT\":\n      return [fft(getParamValue(\"x\", e, t, a))];\n\n    case \"IFFT\":\n      return [ifft(getParamValue(\"x\", e, t, a))];\n\n    case \"RFFT\":\n      return [rfft(getParamValue(\"x\", e, t, a))];\n\n    case \"IRFFT\":\n      return [irfft(getParamValue(\"x\", e, t, a))];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n},\n    executeOp$15 = function executeOp$15(e, t, a) {\n  switch (e.op) {\n    case \"Cast\":\n      return [cast(getParamValue(\"x\", e, t, a), getParamValue(\"dtype\", e, t, a))];\n\n    case \"ExpandDims\":\n      var r = getParamValue(\"axis\", e, t, a);\n      return [expandDims(getParamValue(\"x\", e, t, a), r)];\n\n    case \"Squeeze\":\n      r = getParamValue(\"axis\", e, t, a);\n      return [squeeze(getParamValue(\"x\", e, t, a), r)];\n\n    case \"Reshape\":\n      return [reshape(getParamValue(\"x\", e, t, a), getParamValue(\"shape\", e, t, a))];\n\n    case \"PadV2\":\n    case \"Pad\":\n      return [pad(getParamValue(\"x\", e, t, a), split$1(getParamValue(\"padding\", e, t, a), 2), getParamValue(\"constantValue\", e, t, a))];\n\n    case \"SpaceToBatchND\":\n      var n = getParamValue(\"blockShape\", e, t, a),\n          s = split$1(getParamValue(\"paddings\", e, t, a), 2);\n      return [spaceToBatchND(getParamValue(\"x\", e, t, a), n, s)];\n\n    case \"BatchToSpaceND\":\n      n = getParamValue(\"blockShape\", e, t, a);\n      var o = split$1(getParamValue(\"crops\", e, t, a), 2);\n      return [batchToSpaceND(getParamValue(\"x\", e, t, a), n, o)];\n\n    case \"DepthToSpace\":\n      var p = getParamValue(\"blockSize\", e, t, a),\n          u = getParamValue(\"dataFormat\", e, t, a).toUpperCase();\n      return [depthToSpace(getParamValue(\"x\", e, t, a), p, u)];\n\n    default:\n      throw TypeError(\"Node type \" + e.op + \" is not implemented\");\n  }\n};\n\nfunction executeOp$16(e, t, a) {\n  var r = function (e, t, a) {\n    switch (e.category) {\n      case \"arithmetic\":\n        return executeOp(e, t, a);\n\n      case \"basic_math\":\n        return executeOp$1(e, t, a);\n\n      case \"control\":\n        return executeOp$2(e, t, a);\n\n      case \"convolution\":\n        return executeOp$3(e, t, a);\n\n      case \"creation\":\n        return executeOp$4(e, t, a);\n\n      case \"dynamic\":\n        return executeOp$5(e, t, a);\n\n      case \"evaluation\":\n        return executeOp$6(e, t, a);\n\n      case \"image\":\n        return executeOp$8(e, t, a);\n\n      case \"graph\":\n        return executeOp$7(e, t, a);\n\n      case \"logical\":\n        return executeOp$9(e, t, a);\n\n      case \"matrices\":\n        return executeOp$10(e, t, a);\n\n      case \"normalization\":\n        return executeOp$11(e, t, a);\n\n      case \"reduction\":\n        return executeOp$12(e, t, a);\n\n      case \"slice_join\":\n        return executeOp$13(e, t, a);\n\n      case \"spectral\":\n        return executeOp$14(e, t, a);\n\n      case \"transformation\":\n        return executeOp$15(e, t, a);\n\n      case \"custom\":\n        var r = getRegisteredOp(e.op);\n        if (r && r.customExecutor) return r.customExecutor(new NodeValueImpl(e, t, a));\n        throw TypeError(\"Custom op \" + e.op + \" is not registered.\");\n\n      default:\n        throw TypeError(\"Unknown op '\" + e.op + \"'. File an issue at https://github.com/tensorflow/tfjs/issues so we can add it, or register a custom execution with tf.registerOp()\");\n    }\n  }(e, t, a);\n\n  return r instanceof Promise ? r.then(function (e) {\n    return [].concat(e);\n  }) : [].concat(r);\n}\n\nvar ExecutionContext = function () {\n  function e(e, t) {\n    this.weightMap = e, this.tensorArrayMap = t, this.rootContext = {\n      id: 0,\n      frameName: \"\",\n      iterationId: 0\n    }, this.contexts = [this.rootContext], this.lastId = 0, this.generateCurrentContextIds();\n  }\n\n  return e.prototype.newFrame = function (e, t) {\n    return {\n      id: e,\n      frameName: t,\n      iterationId: 0\n    };\n  }, Object.defineProperty(e.prototype, \"currentContext\", {\n    get: function get() {\n      return this.contexts;\n    },\n    set: function set(e) {\n      this.contexts !== e && (this.contexts = e, this.generateCurrentContextIds());\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"currentContextId\", {\n    get: function get() {\n      return this._currentContextIds[0];\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"currentContextIds\", {\n    get: function get() {\n      return this._currentContextIds;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), e.prototype.generateCurrentContextIds = function () {\n    for (var e = [], t = 0; t < this.contexts.length - 1; t++) {\n      var a = this.contexts.slice(0, this.contexts.length - t);\n      e.push(this.contextIdforContexts(a));\n    }\n\n    e.push(\"\"), this._currentContextIds = e;\n  }, e.prototype.contextIdforContexts = function (e) {\n    return e ? e.map(function (e) {\n      return 0 === e.id && 0 === e.iterationId ? \"\" : e.frameName + \"-\" + e.iterationId;\n    }).join(\"/\") : \"\";\n  }, e.prototype.enterFrame = function (e) {\n    this.contexts && (this.lastId++, this.contexts = this.contexts.slice(), this.contexts.push(this.newFrame(this.lastId, e)), this._currentContextIds.unshift(this.contextIdforContexts(this.contexts)));\n  }, e.prototype.exitFrame = function () {\n    if (!(this.contexts && this.contexts.length > 1)) throw new Error(\"Cannot exit frame, the context is empty\");\n    this.contexts = this.contexts.slice(), this.contexts.splice(-1), this.currentContextIds.shift();\n  }, e.prototype.nextIteration = function () {\n    if (!(this.contexts && this.contexts.length > 0)) throw new Error(\"Cannot increase frame iteration, the context is empty\");\n    this.contexts = this.contexts.slice(), this.lastId++;\n    var e = Object.assign({}, this.contexts[this.contexts.length - 1]);\n    e.iterationId += 1, e.id = this.lastId, this.contexts.splice(-1, 1, e), this._currentContextIds.splice(0, 1, this.contextIdforContexts(this.contexts));\n  }, e.prototype.getWeight = function (e) {\n    return this.weightMap[e];\n  }, e.prototype.addTensorArray = function (e) {\n    this.tensorArrayMap[e.id] = e;\n  }, e.prototype.getTensorArray = function (e) {\n    return this.tensorArrayMap[e];\n  }, e;\n}();\n\nfunction getExecutionSubgraph(e, t, a) {\n  for (var r = new Set(), n = [], s = null, o = null, p = new Set(), u = t.slice(); u.length > 0;) {\n    var i = u.pop();\n    (isControlFlow(i) || isDynamicShape(i)) && null == s && (o = (s = i).children.map(function (e) {\n      return e.name;\n    }).filter(function (e) {\n      return r.has(e);\n    })), r.add(i.name), null == a[i.name] && null == e[i.name] && (0 !== i.inputs.length ? i.inputs.forEach(function (e) {\n      p.has(e.name) || (p.add(e.name), u.push(e));\n    }) : n.push(i.name));\n  }\n\n  return {\n    inputs: e,\n    outputs: t,\n    usedNodes: r,\n    missingInputs: n,\n    dynamicNode: s,\n    syncInputs: o\n  };\n}\n\nfunction getNodesInTopologicalOrder(e, t, a) {\n  var r = a.usedNodes,\n      n = a.inputs,\n      s = [];\n  Object.keys(n).map(function (t) {\n    return e.nodes[t];\n  }).forEach(function (e) {\n    r.has(e.name) && s.push(e);\n  }), e.weights.forEach(function (e) {\n    r.has(e.name) && s.push(e);\n  });\n\n  for (var o = new Set(), p = []; s.length > 0;) {\n    var u = s.pop();\n    o.add(u.name), t[u.name] || p.push(u), u.children.forEach(function (e) {\n      !o.has(e.name) && r.has(e.name) && e.inputs.every(function (e) {\n        return o.has(e.name);\n      }) && s.push(e);\n    });\n  }\n\n  return p;\n}\n\nvar CONTROL_FLOW_OPS = [\"Switch\", \"Merge\", \"Enter\", \"Exit\", \"NextIteration\"],\n    DYNAMIC_SHAPE_OPS = [\"NonMaxSuppressionV2\", \"NonMaxSuppressionV3\", \"Where\"];\n\nfunction isControlFlow(e) {\n  return CONTROL_FLOW_OPS.indexOf(e.op) >= 0;\n}\n\nfunction isDynamicShape(e) {\n  return DYNAMIC_SHAPE_OPS.indexOf(e.op) >= 0;\n}\n\nvar GraphExecutor = function () {\n  function e(e) {\n    this.graph = e, this.compiledMap = new Map(), this._weightMap = {}, this.SEPERATOR = \",\", this.placeholders = e.placeholders, this._outputs = e.outputs;\n  }\n\n  return Object.defineProperty(e.prototype, \"weightMap\", {\n    get: function get() {\n      return this._weightMap;\n    },\n    set: function set(e) {\n      var t = Object.keys(e).map(function (t) {\n        return e[t].map(function (e) {\n          return e.id;\n        });\n      });\n      this.weightIds = [].concat.apply([], t), this._weightMap = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"inputs\", {\n    get: function get() {\n      return this.placeholders.map(function (e) {\n        return {\n          name: e.name,\n          shape: e.attrParams.shape ? e.attrParams.shape.value : void 0,\n          dtype: e.attrParams.dtype ? e.attrParams.dtype.value : void 0\n        };\n      });\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"outputs\", {\n    get: function get() {\n      return this._outputs.map(function (e) {\n        return {\n          name: e.name,\n          shape: e.attrParams.shape ? e.attrParams.shape.value : void 0,\n          dtype: e.attrParams.dtype ? e.attrParams.dtype.value : void 0\n        };\n      });\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"inputNodes\", {\n    get: function get() {\n      return this.placeholders.map(function (e) {\n        return e.name;\n      });\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"outputNodes\", {\n    get: function get() {\n      return this.outputs.map(function (e) {\n        return e.name;\n      });\n    },\n    enumerable: !0,\n    configurable: !0\n  }), e.prototype.getCompilationKey = function (e, t) {\n    var a = e.map(function (e) {\n      return e.name;\n    }).sort(),\n        r = t.map(function (e) {\n      return e.name;\n    }).sort();\n    return a.join(this.SEPERATOR) + \"--\" + r.join(this.SEPERATOR);\n  }, e.prototype.compile = function (e, t) {\n    var a = getExecutionSubgraph(e, t, this.weightMap),\n        r = a.missingInputs,\n        n = a.dynamicNode,\n        s = a.syncInputs;\n    if (null != n) throw new Error(\"This execution contains the node '\" + n.name + \"', which has the dynamic op '\" + n.op + \"'. Please use model.executeAsync() instead. Alternatively, to avoid the dynamic ops, specify the inputs [\" + s + \"]\");\n\n    if (r.length > 0) {\n      var o = t.map(function (e) {\n        return e.name;\n      }),\n          p = Object.keys(e);\n      throw new Error(\"Cannot compute the outputs [\" + o + \"] from the provided inputs [\" + p + \"]. Missing the following inputs: [\" + r + \"]\");\n    }\n\n    return getNodesInTopologicalOrder(this.graph, this.weightMap, a);\n  }, e.prototype.execute = function (e, t) {\n    var a = this,\n        r = Object.keys(e).sort();\n    this.checkInputs(e), this.checkInputShapeAndType(e), this.checkOutputs(t);\n    var n = r.map(function (e) {\n      return a.graph.nodes[e];\n    }),\n        s = t.map(function (e) {\n      return a.graph.nodes[parseNodeName(e)[0]];\n    }),\n        o = this.getCompilationKey(n, s),\n        p = this.compiledMap.get(o);\n    null == p && (p = this.compile(e, s), this.compiledMap.set(o, p));\n    var u = {};\n    return tidy(function () {\n      var r = new ExecutionContext(a._weightMap, u),\n          n = _assign({}, a.weightMap);\n\n      Object.keys(e).forEach(function (t) {\n        n[t] = [e[t]];\n      });\n\n      for (var s = a.getFrozenTensorIds(n), o = {}, i = 0; i < p.length; i++) {\n        var m = p[i];\n\n        if (!n[m.name]) {\n          var l = executeOp$16(m, n, r);\n          if (l instanceof Promise) throw new Error(\"The execution of the op '\" + m.op + \"' returned a promise. Please use model.executeAsync() instead.\");\n          n[m.name] = l, a.checkTensorForDisposal(m.name, m, n, r, s, t, o);\n        }\n      }\n\n      return t.map(function (e) {\n        return getTensor(e, n, r);\n      });\n    });\n  }, e.prototype.getFrozenTensorIds = function (e) {\n    var t = [].concat.apply([], Object.keys(e).map(function (t) {\n      return e[t];\n    }).map(function (e) {\n      return e.map(function (e) {\n        return e.id;\n      });\n    }));\n    return new Set(t);\n  }, e.prototype.checkTensorForDisposal = function (e, t, a, r, n, s, o) {\n    \"control\" !== t.category && -1 === s.indexOf(e) && (a[e].forEach(function (e) {\n      null != e && (o[e.id] = (o[e.id] || 0) + t.children.length);\n    }), t.inputs.forEach(function (e) {\n      if (\"control\" !== e.category) {\n        var t = getTensorsForCurrentContenxt(e.name, a, r);\n        null != t && t.forEach(function (e) {\n          if (e && !n.has(e.id)) {\n            var t = o[e.id];\n            1 === t ? (e.dispose(), delete o[e.id]) : null != t && o[e.id]--;\n          }\n        });\n      }\n    }));\n  }, e.prototype.executeAsync = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var a,\n          r,\n          n,\n          s,\n          o,\n          p,\n          u = this;\n      return __generator(this, function (i) {\n        switch (i.label) {\n          case 0:\n            return this.checkInputs(e), this.checkInputShapeAndType(e), this.checkOutputs(t), a = {}, r = new ExecutionContext(this._weightMap, a), [4, this.executeWithControlFlow(e, r, t)];\n\n          case 1:\n            return n = i.sent(), s = t.map(function (e) {\n              return getTensor(e, n, r);\n            }), o = new Set(s.map(function (e) {\n              return e.id;\n            })), p = new Set(Object.keys(e).map(function (t) {\n              return e[t].id;\n            })), Object.keys(n).forEach(function (e) {\n              n[e].forEach(function (e) {\n                !e || e.isDisposed || o.has(e.id) || p.has(e.id) || -1 !== u.weightIds.indexOf(e.id) || e.dispose();\n              });\n            }), [2, s];\n        }\n      });\n    });\n  }, e.prototype.executeWithControlFlow = function (e, t, a) {\n    return __awaiter(this, void 0, void 0, function () {\n      var r,\n          n,\n          s,\n          o,\n          p,\n          u,\n          i,\n          m,\n          l,\n          c,\n          d,\n          y,\n          f,\n          g,\n          h,\n          N,\n          x = this;\n      return __generator(this, function (V) {\n        switch (V.label) {\n          case 0:\n            r = Object.keys(e), n = r.map(function (e) {\n              return x.graph.nodes[e];\n            }), s = a.map(function (e) {\n              return x.graph.nodes[parseNodeName(e)[0]];\n            }), o = getExecutionSubgraph(e, s, this.weightMap), p = o.usedNodes, u = o.missingInputs, i = o.dynamicNode, m = o.syncInputs, l = n.concat(this.graph.weights).map(function (e) {\n              return {\n                node: e,\n                contexts: t.currentContext\n              };\n            }), c = _assign({}, this.weightMap), Object.keys(e).forEach(function (t) {\n              c[t] = [e[t]];\n            }), d = {}, y = this.getFrozenTensorIds(c), f = {}, V.label = 1;\n\n          case 1:\n            return l.length > 0 ? (g = this.processStack(n, l, t, c, f, y, a, d, p), [4, Promise.all(g)]) : [3, 3];\n\n          case 2:\n            return V.sent(), [3, 1];\n\n          case 3:\n            if (null == i && console.warn(\"This model execution did not contain any nodes with control flow or dynamic output shapes. You can use model.execute() instead.\"), (h = s.filter(function (e) {\n              return !isControlFlow(e) && !getTensor(e.name, c, t);\n            }).map(function (e) {\n              return e.name;\n            })).length > 0) throw N = \"\", null != i && (N = \"Alternatively, to avoid the dynamic ops, use model.execute() and specify the inputs [\" + m + \"]\"), new Error(\"Cannot compute the outputs [\" + h + \"] from the provided inputs [\" + r + \"]. Consider providing the following inputs: [\" + u + \"]. \" + N);\n            return [2, c];\n        }\n      });\n    });\n  }, e.prototype.processStack = function (e, t, a, r, n, s, o, p, u) {\n    for (var i = this, m = [], l = function l() {\n      var l = t.pop();\n      a.currentContext = l.contexts;\n      var d = \"\";\n\n      if (\"Enter\" === l.node.op && getParamValue(\"isConstant\", l.node, r, a) && (d = getNodeNameAndIndex(l.node.name, a)[0]), -1 === e.indexOf(l.node)) {\n        var y = executeOp$16(l.node, r, a);\n        d || (d = getNodeNameAndIndex(l.node.name, a)[0]);\n        var f = a.currentContext;\n        y instanceof Promise ? m.push(y.then(function (e) {\n          return r[d] = e, a.currentContext = f, i.checkTensorForDisposal(d, l.node, r, a, s, o, p), i.processChildNodes(l.node, t, a, r, n, u), e;\n        })) : (r[d] = y, c.checkTensorForDisposal(d, l.node, r, a, s, o, p), c.processChildNodes(l.node, t, a, r, n, u));\n      } else c.processChildNodes(l.node, t, a, r, n, u);\n    }, c = this; t.length > 0;) {\n      l();\n    }\n\n    return m;\n  }, e.prototype.processChildNodes = function (e, t, a, r, n, s) {\n    e.children.forEach(function (e) {\n      var o = getNodeNameAndIndex(e.name, a)[0];\n      !n[o] && s.has(e.name) && (\"Merge\" === e.op ? e.inputNames.some(function (e) {\n        return !!getTensor(e, r, a);\n      }) && (n[o] = !0, t.push({\n        contexts: a.currentContext,\n        node: e\n      })) : e.inputNames.every(function (e) {\n        return !!getTensor(e, r, a);\n      }) && (n[o] = !0, t.push({\n        contexts: a.currentContext,\n        node: e\n      })));\n    });\n  }, e.prototype.dispose = function () {\n    var e = this;\n    Object.keys(this.weightMap).forEach(function (t) {\n      return e.weightMap[t].forEach(function (e) {\n        return e.dispose();\n      });\n    });\n  }, e.prototype.checkInputShapeAndType = function (e) {\n    var t = this;\n    Object.keys(e).forEach(function (a) {\n      var r = e[a],\n          n = t.graph.nodes[a];\n\n      if (n.attrParams.shape && n.attrParams.shape.value) {\n        var s = n.attrParams.shape.value,\n            o = s.length === r.shape.length && r.shape.every(function (e, t) {\n          return -1 === s[t] || s[t] === e;\n        });\n        util.assert(o, function () {\n          return \"The shape of dict['\" + n.name + \"'] provided in model.execute(dict) must be [\" + s + \"], but was [\" + r.shape + \"]\";\n        });\n      }\n\n      n.attrParams.dtype && n.attrParams.dtype.value && util.assert(r.dtype === n.attrParams.dtype.value, function () {\n        return \"The dtype of dict['\" + n.name + \"'] provided in model.execute(dict) must be \" + n.attrParams.dtype.value + \", but was \" + r.dtype;\n      });\n    });\n  }, e.prototype.checkInputs = function (e) {\n    var t = this,\n        a = Object.keys(e).filter(function (e) {\n      return !t.graph.nodes[e];\n    });\n    if (a.length > 0) throw new Error(\"The dict provided in model.execute(dict) has keys: [\" + a + \"] that are not part of graph\");\n  }, e.prototype.checkOutputs = function (e) {\n    var t = this;\n    e.forEach(function (e) {\n      var a = parseNodeName(e)[0];\n      if (!t.graph.nodes[a]) throw new Error(\"The output '\" + e + \"' is not found in the graph\");\n    });\n  }, e;\n}(),\n    TFHUB_SEARCH_PARAM = \"?tfjs-format=file\",\n    DEFAULT_MODEL_NAME = \"model.json\",\n    GraphModel = function () {\n  function e(e, t) {\n    void 0 === t && (t = {}), this.modelUrl = e, this.loadOptions = t, this.version = \"n/a\", null == t && (this.loadOptions = {});\n  }\n\n  return Object.defineProperty(e.prototype, \"modelVersion\", {\n    get: function get() {\n      return this.version;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"inputNodes\", {\n    get: function get() {\n      return this.executor.inputNodes;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"outputNodes\", {\n    get: function get() {\n      return this.executor.outputNodes;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"inputs\", {\n    get: function get() {\n      return this.executor.inputs;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"outputs\", {\n    get: function get() {\n      return this.executor.outputs;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(e.prototype, \"weights\", {\n    get: function get() {\n      return this.executor.weightMap;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), e.prototype.findIOHandler = function () {\n    var e = this.modelUrl;\n    if (null != e.load) this.handler = e;else if (null != this.loadOptions.requestInit) this.handler = io.browserHTTPRequest(e, this.loadOptions);else {\n      var t = io.getLoadHandlers(e, this.loadOptions.onProgress);\n      if (0 === t.length) t.push(io.browserHTTPRequest(e, this.loadOptions));else if (t.length > 1) throw new Error(\"Found more than one (\" + t.length + \") load handlers for URL '\" + [e] + \"'\");\n      this.handler = t[0];\n    }\n  }, e.prototype.load = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      var e, t, a;\n      return __generator(this, function (r) {\n        switch (r.label) {\n          case 0:\n            if (this.findIOHandler(), null == this.handler.load) throw new Error(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");\n            return [4, this.handler.load()];\n\n          case 1:\n            return e = r.sent(), t = e.modelTopology, this.version = t.versions.producer + \".\" + t.versions.minConsumer, a = io.decodeWeights(e.weightData, e.weightSpecs), this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(t)), this.executor.weightMap = this.convertTensorMapToTensorsMap(a), [2, !0];\n        }\n      });\n    });\n  }, e.prototype.predict = function (e, t) {\n    return this.execute(e, this.outputNodes);\n  }, e.prototype.normalizeInputs = function (e) {\n    if (!(e instanceof Tensor || Array.isArray(e))) return e;\n    if ((e = Array.isArray(e) ? e : [e]).length !== this.inputNodes.length) throw new Error(\"Input tensor count mismatch,the graph model has \" + this.inputNodes.length + \" placeholders, while there are \" + e.length + \" input tensors.\");\n    return this.inputNodes.reduce(function (t, a, r) {\n      return t[a] = e[r], t;\n    }, {});\n  }, e.prototype.normalizeOutputs = function (e) {\n    return e = e || this.outputNodes, Array.isArray(e) ? e : [e];\n  }, e.prototype.execute = function (e, t) {\n    e = this.normalizeInputs(e), t = this.normalizeOutputs(t);\n    var a = this.executor.execute(e, t);\n    return a.length > 1 ? a : a[0];\n  }, e.prototype.executeAsync = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var a;\n      return __generator(this, function (r) {\n        switch (r.label) {\n          case 0:\n            return e = this.normalizeInputs(e), t = this.normalizeOutputs(t), [4, this.executor.executeAsync(e, t)];\n\n          case 1:\n            return [2, (a = r.sent()).length > 1 ? a : a[0]];\n        }\n      });\n    });\n  }, e.prototype.convertTensorMapToTensorsMap = function (e) {\n    return Object.keys(e).reduce(function (t, a) {\n      return t[a] = [e[a]], t;\n    }, {});\n  }, e.prototype.dispose = function () {\n    this.executor.dispose();\n  }, e;\n}();\n\nfunction loadGraphModel(e, t) {\n  return void 0 === t && (t = {}), __awaiter(this, void 0, void 0, function () {\n    var a;\n    return __generator(this, function (r) {\n      switch (r.label) {\n        case 0:\n          if (null == e) throw new Error(\"modelUrl in loadGraphModel() cannot be null. Please provide a url or an IOHandler that loads the model\");\n          return null == t && (t = {}), t.fromTFHub && null == e.load && (e.endsWith(\"/\") || (e += \"/\"), e = \"\" + e + DEFAULT_MODEL_NAME + TFHUB_SEARCH_PARAM), [4, (a = new GraphModel(e, t)).load()];\n\n        case 1:\n          return r.sent(), [2, a];\n      }\n    });\n  });\n}\n\nvar version = \"1.3.0\";\nexport { GraphModel, loadGraphModel, deregisterOp, registerOp, version as version_converter };","map":null,"metadata":{},"sourceType":"module"}